{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "562f7f8e-5874-46c3-8f40-db445a980984",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schema Evolution in Merge Operations\n",
    "This notebook provides an example of how to perform schema evolution in merge operations for the [2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE](https://github.com/CSSEGISandData/COVID-19) dataset.   This primer example allows you to create a Delta Lake table (instead of native Parquet) to track the changes of the this dataset to support the [Johns Hopkins COVID-19 Data Analysis Sample Notebook](https://github.com/databricks/tech-talks/blob/master/samples/JHU%20COVID-19%20Analysis.html).\n",
    "\n",
    "The data is updated in the `/databricks-datasets/COVID/CSSEGISandData/` location regularly so you can access the data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e908977a-4862-4c17-b968-65674d8a1740",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark\n",
    "from pyspark.sql.functions import input_file_name, lit, col\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4853acc-3dbc-4db9-86a9-def1cd92df99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Different DataFrames for Different Schemas\n",
    "As noted in the [Johns Hopkins COVID-19 Analysis](https://github.com/databricks/tech-talks/blob/master/samples/JHU%20COVID-19%20Analysis.html) notebook, as of this writing, there are three different schemas for this dataset; for this example, we will focus on the last two schema changes.\n",
    "\n",
    "| id | Schema String List | Date Range | \n",
    "| -- | ------------------ | ---------- |\n",
    "| 1 | `Province/State Country/Region Last Update Confirmed Deaths Recovered Latitude Longitude` | 03-01-2020 to 03-21-2020 |\n",
    "| 2 | `FIPS Admin2 Province_State Country_Region Last_Update Lat Long_ Confirmed Deaths Recovered Active Combined_Key` | 03-22-2020 to current |\n",
    "\n",
    "The following is an example of:\n",
    "* How to run merge operations with schema evolution for representative files of the different schemas\n",
    "* We will focus on only Washington State data for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08bf3da0-b41f-40e4-a6e5-0618cc21fc77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "#   Two representative files of the two different schemas\n",
    "file_1 = '/databricks-datasets/COVID/CSSEGISandData/csse_covid_19_data/csse_covid_19_daily_reports/03-21-2020.csv'\n",
    "file_2 = '/databricks-datasets/COVID/CSSEGISandData/csse_covid_19_data/csse_covid_19_daily_reports/04-11-2020.csv'\n",
    "\n",
    "# Schema 1: Representing datasets between 2020-03-01 and 2020-03-21 (8 columns originally)\n",
    "#   Renaming some of the columns for better standardization\n",
    "old_data = (spark.read\n",
    "          .option(\"inferSchema\", True)\n",
    "          .option(\"header\", True)\n",
    "          .csv(file_1)\n",
    "          .withColumnRenamed(\"Last Update\", \"Last_Update\")\n",
    "          .withColumnRenamed(\"Province/State\", \"Province_State\")\n",
    "          .withColumnRenamed(\"Country/Region\", \"Country_Region\")\n",
    "          .withColumn(\"process_date\", lit('2020-03-21'))   # Date determined by the filename, manually entered in this example\n",
    "          .withColumn(\"level\", lit(2))                     # Specify the level: 1 - Country, 2 - Province/State, 3 - County\n",
    "          .where(\"Province_State == 'Washington'\"))        # Filter by only Washington State (expected output: 1 row)\n",
    "\n",
    "# Schema 2: Latest schema representing data from 2020-03-22 onwards (12 columns originally)\n",
    "#   Renaming some of the columns for better standardization\n",
    "new_data = (spark.read\n",
    "          .option(\"inferSchema\", True)\n",
    "          .option(\"header\", True)\n",
    "          .csv(file_2)\n",
    "          .withColumnRenamed(\"Lat\", \"Latitude\")\n",
    "          .withColumnRenamed(\"Long_\", \"Longitude\")\n",
    "          .withColumn(\"process_date\", lit('2020-04-11'))    # Date determined by the filename, manually entered in this example\n",
    "          .withColumn(\"level\", lit(3))                      # Specify the level: 1 - Country, 2 - Province/State, 3 - County\n",
    "          .where(\"Province_State == 'Washington'\"))         # Filter by only Washington State (expected output: 39 rows)\n",
    "\n",
    "# Notes: Expand each DataFrame below to review the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a50d9831-7b13-4809-beaa-9a0d3ec43e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Old Data Schema\n",
    "old_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a440fa87-5056-42bb-994b-8780053fe5a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New Data Schema\n",
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5790b57c-c377-4f85-9a79-64266384a7b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The difference between these two schemas are:\n",
    "* Columns that were renamed: `Province/State -> Province_State`, `Country/Region -> Country_Region`, `Latitude -> Lat`, `Longitude -> Long_`.  To resolve this issue, we've standardized the column names\n",
    "* Columns that were added:\n",
    "  * `FIPS`: The Federal Information Processing Standard Publication 6-4 (FIPS 6-4) was a five-digit Federal Information Processing Standards code which uniquely identified counties and county equivalents in the United States, certain U.S. possessions, and certain freely associated states ([source](https://en.wikipedia.org/wiki/FIPS_county_code)) that is commonly used for US topological maps. This code has been supplanted with the [INCITS 31 â€“ 2009](https://en.wikipedia.org/wiki/International_Committee_for_Information_Technology_Standards) codes. \n",
    "  * `Admin2`: Contains more granular region name, e.g. within the United States this would be the county name.\n",
    "  * `Combined_Key`: Comma concatenation of `Admin2`, `Province_State`, `Country_Region`.\n",
    "  * `Active`: Active COVID-19 cases\n",
    "  \n",
    "We also added the following columns:\n",
    "* `process_date`: The date of the confirmed cases (when the tests were processed) which is not in the data itself but within the file name\n",
    "* `level`: Describing the level of granuality of the data: `old_data` is at the state/province level (`level = 2`) while `new_data` is at the county level (`level = 3`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcfd04f-69c1-4d39-9b6f-d5b84c7efedd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create File Path for Delta Lake Table\n",
    "Removing if exists and creating the following file path for our Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0bd006-ccd2-492f-bec4-93a2f3cf11fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "rm -fR /dbfs/tmp/dennylee/COVID/df_jhu/ && mkdir -p /dbfs/tmp/dennylee/COVID/df_jhu/ && ls -lsgA /dbfs/tmp/dennylee/COVID/df_jhu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36fcd8d8-5396-40c3-bd98-e2b0967399f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create our initial Delta Lake table\n",
    "DELTA_PATH = \"/tmp/dennylee/COVID/df_jhu/\"\n",
    "old_data.write.format(\"delta\").save(DELTA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ee3bbee-db99-417c-bb6e-852457e3b9c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Simulate an Updated Entry\n",
    "In this example scenario, on April 27th, 2020 we needed to updated the `Last_Updated` column for data for March 21st, 2020 which was stored in the older schema (`old_data`).\n",
    "\n",
    "But this **update** entry is included in the `new_data` with a newer schema including:\n",
    "* An updated `Last_Update` value\n",
    "* Including the FIPS county code for Washington State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7487b577-6f13-4b8f-b1f0-5e73a1400c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate an Updated Entry\n",
    "items = [(53, '', 'Washington', 'US', '2020-04-27T19:00:00', 47.4009, -121.4905, 1793, 94, 0, '', '', '2020-03-21', 2)]\n",
    "cols = ['FIPS', 'Admin2', 'Province_State', 'Country_Region', 'Last_Update', 'Latitude', 'Longitude', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'Combined_Key', 'process_date', 'level']\n",
    "simulated_update = spark.createDataFrame(items, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b825456b-be86-4203-bc33-ea127bd6f342",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add this updated entry into the new_data\n",
    "new_data = new_data.union(simulated_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b086e17f-4f5c-471c-a488-84f12a07c50e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Review the Data\n",
    "Let's review the data for our **Schema Evolution in Merge Operations** example:\n",
    "1. `old_data` contains the original schema \n",
    "2. `new_data` contains a new schema which includes the columns: `FIPS`, `Admin2`, `Active`, `Combined_Key`\n",
    "3. `new_data` also contains our simulated update entry originally inserted with the old schema (`old_data`) which includes an updated `Last_Update` and `FIPS` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7953de68-d9a6-493d-8d53-5318da5f722e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows for old_data: %s, new_data: %s \" % (old_data.count(), new_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e128ba-b3c9-4256-aa4e-93a5e87aeece",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In the old data, there is a single row with Washington state with 10 columns\n",
    "display(old_data.where(col(\"Province_State\") == \"Washington\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2122b67f-5e61-4a8d-a250-15c9963e8b21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In the second DataFrame, there are:\n",
    "# - multiple rows with Washington State\n",
    "# - It now contains 14 columns\n",
    "# - An additional row for , we've updated the `Last_Update` value\n",
    "# - Note this row has level = 2 (other rows has level = 3)\n",
    "display(new_data.where(col(\"Province_State\") == \"Washington\").sort(col(\"FIPS\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2bf816-e7c3-440e-bf5c-bce326ecba34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schema Evolution?\n",
    "We could potentially use `new_data.write.option(\"merge\", \"true\").mode(\"append\").save(path)` to merge the schemas but `new_data` also contains data that has to be updated in the original table.  One approach could be that you:\n",
    "* Run the `merge` as one operation\n",
    "* Run the `schema evolution` as another operation\n",
    "\n",
    "Or, we could do this as a single operation by **[Automatic Schema Evolution](https://docs.delta.io/latest/delta-update.html#automatic-schema-evolution)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e88c4b-d6e1-4df5-9942-64b9fe92b56e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Automatic Schema Evolution\n",
    "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8771be-ffbc-486f-893f-72f50ebd0b48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, DELTA_PATH)\n",
    "\n",
    "# Schema Evolution with a Merge Operation\n",
    "deltaTable.alias(\"t\").merge(\n",
    "  new_data.alias(\"s\"),\n",
    "  \"s.process_date = t.process_date AND s.province_state = t.province_state AND s.country_region = t.country_region AND s.level = t.level\"\n",
    ").whenMatchedUpdateAll(  \n",
    ").whenNotMatchedInsertAll(\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc35ec1e-b7fc-4fde-9cf4-04a4d6d2910d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Review the Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "794785d3-e9c5-4542-aea5-b8b3290ee9c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = spark.read.format(\"delta\").load(DELTA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb1adb0-f2cd-4acf-bfb5-38d4b7ae8c97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows: %s\" % df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e25e2e-9431-4012-9148-9a2ca171fe57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Recall this is expected as the `old_data` contained 1 row, `new_data` contained 40 rows, <br/>\n",
    "but one of the rows contained a simulated row to update the values previously inserted into the Delta Table by `old_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1542e8d3-b8b6-4d3e-9794-204433acf72a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.sort(col(\"FIPS\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "babbfe2e-ce8b-4888-a766-d9ffab9fe4b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As expected, there are 40 rows with the `level = 2` containing an updated `Last_Update` value thus\n",
    "* The Delta Lake table schema evolved from 10 columns to 14 columns\n",
    "* A single row value was updated \n",
    "\n",
    "All of this occured in a single atomic operation as noted in the history below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264f8422-36ad-434c-9b82-3e225f8051a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(deltaTable.history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a44b2127-3d7a-472f-8f60-8be44990f915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Review the Operational Metrics\n",
    "Note the `operationMetrics` column for this example:\n",
    "* `numTargetRowsInserted: 39` were the number of rows added (with the new schema)\n",
    "* `numTargetRowsUpdated: 1` were the number of rows updated (with the old schema)\n",
    "\n",
    "**Important:** Don't forget to review the SQL tab of the SQL UI to better understand the internals (it should look similar to the animated GIF below)\n",
    "\n",
    "![](https://raw.githubusercontent.com/databricks/tech-talks/master/images/schema-evolution_merge-operation-spark-ui-sql-tab-5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53a81765-de3b-45a4-9a2e-3c26251d65e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Review the Transaction Log\n",
    "Let's take a quick look at the transaction log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ff7a95d-3999-494e-9431-9a1e6f0a217f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -lsgA /dbfs/tmp/dennylee/COVID/df_jhu/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "204f866d-f10c-4b77-be0a-faa9c00d5c0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tl_1 = spark.read.json(\"/tmp/dennylee/COVID/df_jhu/_delta_log/00000000000000000001.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5071ae79-a7f9-4745-ba8b-9fae3bf0dbbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Commit Information: Note the operations metrics\n",
    "display(tl_1.select(\"commitInfo\").where(\"commitInfo is not null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a792c9-6c6c-488c-8993-5610c33663e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add Information:\n",
    "#  Notice the two rows under `stats`: one noting the 39 records inserted and one noting the 1 record updated\n",
    "display(tl_1.select(\"add\").where(\"add is not null\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e68bb9-bf61-43ca-a316-1e3e1be8f395",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Join the community!\n",
    "\n",
    "\n",
    "* [Delta Lake on GitHub](https://github.com/delta-io/delta)\n",
    "* [Delta Lake Slack Channel](https://delta-users.slack.com/) ([Registration Link](https://join.slack.com/t/delta-users/shared_invite/enQtNTY1NDg0ODcxOTI1LWJkZGU3ZmQ3MjkzNmY2ZDM0NjNlYjE4MWIzYjg2OWM1OTBmMWIxZTllMjg3ZmJkNjIwZmE1ZTZkMmQ0OTk5ZjA))\n",
    "* [Public Mailing List](https://groups.google.com/forum/#!forum/delta-users)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Schema Evolution in Merge Operations",
   "widgets": {}
  },
  "name": "Schema Evolution in Merge Operations",
  "notebookId": 6697717
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
